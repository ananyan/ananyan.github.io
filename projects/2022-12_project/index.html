<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Ananya Nandy | Design space exploration in VR</title> <meta name="author" content="Ananya Nandy"> <meta name="description" content="design, research"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@/dist/css/bootstrap.min.css" rel="stylesheet" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@/css/mdb.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@/css/all.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@/css/academicons.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/face-logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ananyan.github.io/projects/2022-12_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ananyan.github.io/"><span class="font-weight-bold">Ananya</span> Nandy</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/AnanyaNandy_CV_0125.pdf">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Design space exploration in VR</h1> <p class="post-description">design, research</p> </header> <article> <h2 id="overview"><u>Overview</u></h2> <p><strong>My Role:</strong> Contributions both as a researcher and developer: generating the designs and creating the pipeline to transfer them between the 3D modeling software and the VR application, creating the VR environments, setting up the data logging for the user study, designing the user study, administering the user study, analyzing the quantitative and qualitative behavioral data<br> <strong>Tools/Skills:</strong> User Study, Unity, Rhino3D/Grasshopper, C#, Python <br> <strong>Timeline:</strong> August 2021 - December 2022 (17 months) <br> <strong>Team:</strong> 5 (MechE, Design, HCI)</p> <h2 id="context"><u>Context</u></h2> <p>Virtual reality enables the creation of realistic environments and the ability to use spatial senses rather than only rely on vision. There is the possibility to use VR tools in a design context, but it is important to avoid simply porting existing workflows into VR without leveraging its unique benefits. Decisions related to designing or purchasing physical objects may benefit from the interactions afforded by VR because it is difficult to convey many aspects of a 3D, physical object using screen-based interfaces.</p> <div class="row"> <div class="w-50 p-3" style="margin:auto"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/ICVR/questheadset-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/ICVR/questheadset-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/ICVR/questheadset-1400.webp"></source> <img class="img-fluid" src="/assets/img/ICVR/questheadset.jpg" alt="Image of Oculus Quest headset"> </picture> </figure> </div> </div> <h2 id="goal"><u>Goal</u></h2> <ul> <li>Prototype interactions that leverage unique aspects of VR to explore a large-scale design space</li> <li>Evaluate the utility of VR for such a task in comparison to screen-based interfaces</li> </ul> <h2 id="outcomes"><u>Outcomes</u></h2> <p>1) Developed interactions that allowed users to narrow down from a space of over 3000 options to a final option in a short amount of time (~15 minutes)</p> <p>2) Evaluated the interactions for their usability, finding that they were generally intuitive, except for the functionality-based approach which users had a hard time understanding and would require more training or a paradigm shift in approach</p> <p>3) Evaluated the interface modality (screen-based and VR), finding that certain aspects of search and exploration are better suited for one interface type over the other</p> <p>4) Identified potential future directions</p> <h2 id="developing-the-interactions"><u>Developing the interactions</u></h2> <p>We used the example of furniture (storage shelving) to demonstrate different interactions that can allow users to better search through a design space.</p> <p>We developed 2 primary interactions:</p> <p>1) A direct manipulation filter that allows users to specify the geometry of their preferred design, using a virtual environment to reference scale</p> <div class="row"> <div class="w-50 p-3" style="margin:auto"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/ICVR/DirectManipulationFilter-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/ICVR/DirectManipulationFilter-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/ICVR/DirectManipulationFilter-1400.webp"></source> <img class="img-fluid" src="/assets/img/ICVR/DirectManipulationFilter.png" alt="figure demonstrating direct manipulation to narrow design space"> </picture> </figure> </div> </div> <p>2) A functionality filter that allows users to sort through designs based on the ability of the shelf to hold their desired items</p> <div class="row"> <div class="w-50 p-3" style="margin:auto"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/ICVR/FunctionalityFilter-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/ICVR/FunctionalityFilter-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/ICVR/FunctionalityFilter-1400.webp"></source> <img class="img-fluid" src="/assets/img/ICVR/FunctionalityFilter.png" alt="figure demonstrating items being placed on shelf to narrow design space"> </picture> </figure> </div> </div> <p>The video below shows the initial version of our interactions, which were presented at CHI 2022 as Late Breaking Work:</p> <div align="center"> <iframe width="560" height="315" src="https://www.youtube.com/embed/3K3QM1NFHqU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <h2 id="iterating-on-the-interactions"><u>Iterating on the interactions</u></h2> <p>Based on pilot testing of people using the interactions, we improved the interactions and added a few additional features that could be evaluated in a user study.</p> <p>1) Users could originally visually filter down by seeing the overlay of transparent shelves that represented the design space and then had to look elsewhere to see the number of designs remaining after filtering. Having easier access to the exact number could help people set their filters more precisely so that they would not over-constrain the space too soon. <strong>Placing the number as a tag on the tool and updating it in real-time</strong> allowed users to get an at-glance summary of the impact of their actions on the resulting design space.</p> <p>2) Users originally made their decisions in an empty virtual environment. However, <strong>adding a room environment</strong> that could simulate how the design would be used in-context was beneficial for further utilizing the spatial benefits of VR. Then, users could place the shelf in the room and walk around to see how it would be used within an actual space.</p> <p>3) Users were embodied as themselves when entering the VR environment. However, another benefit of VR is the ability to change embodiment. In order to facilitate design from other perspectives, we added a small feature that allowed users to <strong>modify their height (taller or shorter)</strong> to better explore how the shelves might be used by people different from themselves.</p> <p>Using those insights, the interactions were refined and then used in a user study (presented at ICED ‘23 and published). An example of using the refined interactions to narrow down to a desired design is shwon below:</p> <div align="center"> <iframe width="560" height="315" src="https://www.youtube.com/embed/VCn3S_4W6OU?si=t_RMg_Vgl76Re6J_" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <p><strong>Skip to Timestamp 01:39 - 02:29</strong> for a demonstration of the Direct Manipulation Filter, which allows gestural specification of geometry<br> <strong>Skip to Timestamp 04:43 - 05:24</strong> for a demonstration of the Functionality Filter, which allows the placement of objects to specify how the design will function (how the shelf will hold items)<br> <strong>Skip to Timestamp 5:45 - 7:00</strong> for a demonstration of using both the Direct Manipulation Filter and Functionality Filter to constrain the design space based on both form and function</p> <h2 id="user-study---quantitative-and-qualitative-evaluation"><u>User study - quantitative and qualitative evaluation</u></h2> <p>In order to evaluate the interactions and the interface modality, we developed <strong>2 tasks for participants</strong> (N=28, each having at least a little design experience) to complete. Participants were given two design tasks: select a shelf to display projects in a Design Institute’s entry and select a shelf for a small library in the lobby. All participants completed the two tasks in the same order, but either completed the first task <strong>using VR or a screen-based version of the interface</strong> with the same interactions (adapted slightly so they wouldn’t be disadvantaged completely by the modality). The order in which the interfaces were used was assigned randomly.</p> <p>We provided training on how to use each interface before allowing participants to complete each task. We collected <strong>action logs</strong> of what participants did, <strong>survey responses after each task</strong>, and an overall <strong>post-study survey</strong>.</p> <h2 id="key-insights-and-impact"><u>Key insights and impact</u></h2> <h4 id="1-logs-reveal-differences-in-use-of-functionality-interaction"><strong>1. Logs reveal differences in use of functionality interaction</strong></h4> <p>Logs of when people used the interactions during the task revealed that some people used the interactions as expected and some people did not. For example, the functionality filter was originally developed so that users could specify high-level abstractions of function first, rather than low-level design parameters. Two broad types of users emerged for this interaction:</p> <p><strong>“Function first”:</strong> Used the functionality filter at the start to narrow design space to fewer, but visually diverse possibilities<br> <strong>“Function checker”:</strong> Increased their use of the functionality filter at the end, often after a final design had been selected (e.g., to check if objects could be placed as desired)</p> <h4 id="2-expressing-function-based-intent-directly-is-challenging-but-an-immersive-interface-may-help"><strong>2. Expressing function-based intent directly is challenging, but an immersive interface may help</strong></h4> <p>Furthermore, the use of the functionality filter revealed where there the interactions are influenced by different interface modalities. While the direct manipulation filters were rated similarly for usefulness across both screen-based and VR modalities, the functionality filter had slightly higher ratings of usefulness when it was used in the VR vs. screen-based interface. At the same time, overall reviews of the functionality filter was mixed - some people found it very useful and others found it not useful at all.</p> <h4 id="3-vr-may-not-support-breadth-of-exploration-but-instead-enable-in-depth-consideration-of-an-options-benefits-and-constraints-eg-by-sense-of-scale-simulating-function"><strong>3. VR may not support breadth of exploration, but instead enable in-depth consideration of an option’s benefits and constraints (e.g., by sense of scale, simulating function)</strong></h4> <p>Additionally, commenting on completing the task in the two different interface modalities overall, participants noted that the screen-based interface let them feel less limited in exploration:</p> <p>“By being able to physically, or in this case, virtually, experience the designs in person gave it a greater sense of how a user would be interacting with the design constrain[ing] heavily the ranges of designs I could consider. <strong>With 2D, I didn’t feel like I was in the space and constrained, which allowed me to look at more designs…</strong>”</p> <p>But the VR interface enabled them to understand the different alternatives and how they would be used better:<br> “Although it took a while to get used to the control, I was able to <strong>better imagine myself using the design</strong> and get a better sense of scale. There were <strong>times where I couldn’t reach the top of the shelve[s]</strong>, and that helped me choose what height shelves I should use.”</p> <p><strong>These insights can be applied to guide decisions about which type of interface modality should be used for search and exploration tasks, both in and outside of design, as well as guide the development of top-down, intent-based expression, which can be challenging for users to conceptualize.</strong></p> <h2 id="next-steps"><u>Next steps</u></h2> <p>While the novel interactions developed here provide insights into different ways to enable design space exploration, there were clearly some challenges. Overall, communicating to participants how the filters worked was challenging because designers are used to being able to create geometries from the bottom-up, rather than specify what they are looking for from the top-down. More work is required to understand how intent specification will change as designers and consumers alike use new types of tools to facilitate creation, search, and exploration. VR demonstrated potential for activating spatial considerations during decision making, but there are still few ways to precisely measure the impacts of such immersion on both experience and outcomes aside from self-reported measures. The action logs gave some insights into how interactions were being used, but a larger sample size is needed to make sense of patterns due to the large variability in what participants do in more open-ended (but more realistic) tasks.</p> <h2 id="reflection"><u>Reflection</u></h2> <p>This was an interesting exploration of how we might want to specify constraints or requirements using modalities other than text. In this case, VR allows an immersive way to specify desired functionality or size, as well as view the designs from different perspectives. However, a challenge with using VR in the way we implemented the interactions is that you cannot see the design from a “bird’s eye view,” which is a convenience of traditional 3D software. In addition, people are not used to interacting with design spaces in this way (at a higher-level of abstraction, based on use and gestural specification of geometry) compared to modifications at a parameter level. Therefore, more work would have to be done to understand how people can create a better mental model such an approach and what capabilities it may enable. Furthermore, this project was done before the more widespread accessibility of text-to-3D models. The approach taken here might lead to interesting considerations of how designers or consumers can better specify what they are looking for when it is difficult to describe in words.</p> <p>See the following publications for more details about this project:<br> <a href="https://dl.acm.org/doi/10.1145/3491101.3519616" rel="external nofollow noopener" target="_blank">GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces (CHI Extended Abstracts 2022)</a><br> <a href="https://ananyan.github.io/assets/pdf/nandy-vrornot-iced.pdf">VR or Not? Investigating Interface Type and User Strategies for Interactive Design Space Exploration (International Conference on Engineering Design 2023)</a></p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ananya Nandy. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@/dist/jquery.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@/dist/umd/popper.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@/dist/js/bootstrap.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@/js/mdb.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@/dist/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>