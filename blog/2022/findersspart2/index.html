<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Ananya Nandy | 3D point cloud classification of engineering designs using graph neural networks</title> <meta name="author" content="Ananya Nandy"> <meta name="description" content="Part II of technical concepts learned for data-driven design"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@/dist/css/bootstrap.min.css" rel="stylesheet" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@/css/mdb.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@/css/all.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@/css/academicons.min.css" integrity="" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/face-logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ananyan.github.io/blog/2022/findersspart2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://ananyan.github.io/"><span class="font-weight-bold">Ananya</span> Nandy</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/AnanyaNandy_CV_0125.pdf">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">3D point cloud classification of engineering designs using graph neural networks</h1> <p class="post-meta">August 1, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <p>The second part of my summer school experiments were done on my own just for fun and to see what worked/did not work outside of the given challenge problems. </p> <hr> <h2 id="motivation">Motivation</h2> <p>While I wanted to explore 3D data, I was also curious about how graph neural networks work and can apply in a design context, leading me to the <a href="https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing" rel="external nofollow noopener" target="_blank">Point Cloud Classification tutorial</a> in PyTorch Geometric. I decided to use the <a href="https://simjeb.github.io/" rel="external nofollow noopener" target="_blank">SimJEB dataset</a>, which consists of 382 jet engine bracket 3D models to test this out, though I was unsure if this would be enough data for a graph neural network.</p> <h2 id="data">Data</h2> <p>Each bracket, excluding those categories labeled as “other,” was converted to a point cloud using the open3d library and 2000 points sampled from the .obj file. The dataset was formatted to PyTorch Geometric data by recording the point positions and the associated bracket category to be used later for testing.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/knngraph_b1.jpg" alt="visual of block like bracket point cloud connected by edges"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/knngraph_b2.jpg" alt="visual of beam like bracket point cloud connected by edges"> </picture> </figure> </div> </div> <div class="caption"> <i>K-nearest neighbors clustering with k = 20 to connect nearest points with edges into a graph.</i> </div> <h2 id="graph-neural-network">Graph Neural Network</h2> <p>The GNN was taken with almost no modification from the Point Cloud Classification tutorial from PyTorch Geometric. PointNet++ implementation summarized from the tutorial:</p> <ol> <li>Group nodes using k-nearest neighbors as shown in the image above (or ball queries). In my case, I used k=20, somewhat arbitrarily.</li> <li>PointNet layers which use message passing based on the hidden features of the point at each layer and the point positions</li> <li>Downsampling to extract global features</li> </ol> <p>The downsampling is used in a normal implementation but I didn’t include it in my initial exploration and instead used the basic version that had already been implemented.</p> <p>The neural network’s architecture is as follows: 2 PointNet layers, which take the 3 input features and map to 32 features, followed by a linear classifier that classifies the output into 5 classes. More details are available in the original PointNet++ paper as well as the tutorial I followed.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/pointnetarch.jpg" alt="layers of neural net: sampling &amp; grouping, pointnet, sampling &amp; grouping, pointnet, output"> </picture> </figure> </div> </div> <div class="caption"> <i>PointNet++ architecture. Not my image: from the PyTorch Geometric PointNet++ tutorial.</i> </div> <h2 id="training">Training</h2> <p>I shuffled the data and split it into approximately a 90/10 ratio for training and testing. I left the default learning rate of 0.01 for the Adam optimizer, the default loss as the Cross Entropy loss, as well as the default batch size of 10. I let the model train for 50 epochs initially, but after observing that the loss was still decreasing, I decided to increase the training to 200 epochs. The initial loss was 1.5779 and by epoch 200 it was 0.4869. However, at around epoch 193, the loss was at its lowest at 0.1033 before it started to increase again. I would have to look more closely at this behavior to understand the exact problem, but it seems there was overfitting.</p> <h2 id="outcome">Outcome</h2> <p>The final accuracy on the test set was 0.5263 and is visualized in the confusion matrix below. Obviously, this accuracy is not very good, though with 5 classes, it is better than random guessing. Additionally, the test set classes are a bit unbalanced, with only one test set example being the arch or butterfly bracket. This is in line with the dataset, which had fewer submitted designs of this type (as they tended to have lower mass but also lower strength). I would need to use the AUC metric instead of accuracy to address this point. However, the results were somewhat expected for several reasons:</p> <ol> <li>The PointNet++ implementation is not complete or tuned, having been taken directly from the tutorial.</li> <li>There are only hundreds of training examples instead of thousands or more.</li> <li>The labels in the dataset were determined qualitatively by the researchers who created the dataset. Unlikely common objects like chairs or tables in ShapeNet, there is actually no ground truth for the “types” of brackets. Therefore, it is possible that a good PointNet++ embedding could result in classification that actually makes more sense based on visual similarity than the human categorization. In this case comparing to the human labels is not a good accuracy measure.</li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/confmat-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/confmat-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/confmat-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/confmat.jpg" alt="full confusion matrix showing true and predicted bracket category labels. important results are summarized in the text"> </picture> </figure> </div> </div> <p>I looked at the confusion matrix more closely to try to understand what the issues might be before thinking about making any changes to the network’s architecture or trying a different approach altogether. There were a couple of things to note just based on the matrix:</p> <ol> <li>The block designs were categorized well, as was the single butterfly design.</li> <li>The flat designs were not terribly categorized, though they were often mistaken for a block design and in rarer cases, an arch or beam design.<br> Visual inspection showed why some brackets may not have been categorized correctly, pointing towards problems when the ground truth is unknown. Below is the example of the bracket in the test set that had a true label as an arch, but was categorized as a block.</li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b415-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b415-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b415-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b415.jpg" alt="Bracket 415 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b417-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b417-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b417-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b417.jpg" alt="Bracket 417 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b316-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b316-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b316-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b316.jpg" alt="Bracket 316 3D model"> </picture> </figure> </div> </div> <div class="caption"> <i>On the left is a 3D model of bracket 415. The true category was arch, but this was categorized as a block. In the middle is bracket 417, an example of an arch category bracket from the training set. On the right is bracket 316, a random example of a block category from the training set. Visually, its categorization of 415 as block does not seem to be completely off base!</i> </div> <p>Another example is several brackets that had true labels as flat, but were categorized as something else.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b39-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b39-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b39-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b39.jpg" alt="Bracket 39 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b28-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b28-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b28-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b28.jpg" alt="Bracket 28 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b198-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b198-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b198-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b198.jpg" alt="Bracket 198 3D model"> </picture> </figure> </div> </div> <div class="caption"> <i>Left: bracket 39, classified as block; middle: bracket 28, classified as beam; right: bracket 198, classified as arch. Below are all brackets that were correctly identified as flat.</i> </div> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b564-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b564-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b564-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b564.jpg" alt="Bracket 564 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b205-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b205-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b205-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b205.jpg" alt="Bracket 205 3D model"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b548-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b548-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b548-1400.webp"></source> <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b548.jpg" alt="Bracket 548 3D model"> </picture> </figure> </div> </div> <div class="caption"> <i>On the left is bracket 564, in the middle is bracket 205, and on the right is bracket 548.</i> </div> <p>Clearly, it is difficult to distinguish the features that put a bracket in one category over another. If an unsupervised approach was taken and the brackets were clustered instead, maybe different categories would have resulted. Furthermore, it is possible that a human might agree with the results of the classifier if there are enough common features with the rest of the category that can be found by eye. Another future approach to explore for this type of data is semi-supervised learning.</p> <h2 id="lessons-learned">Lessons learned</h2> <p>This example from engineering design was particularly interesting because I would say the classifier appeared to work relatively fine given that the “true” labels were actually quite uncertain. In addition, I did not change the neural net’s architecture or hyperparameters so it is possible that things could be improved in that way. Of course, there is a lot left to be learned about different types of neural networks or other methods of learning a compact representation of 3D models (maybe, PointNet++ is not the best for the amount of data and the diversity in the global shape of the meshes). However, finding something that better represents this type of design-specific dataset could be useful, for example, to understand visual similarity of forms.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ananya Nandy. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@/dist/jquery.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@/dist/umd/popper.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@/dist/js/bootstrap.min.js" integrity="" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@/js/mdb.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@/dist/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>