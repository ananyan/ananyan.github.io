<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ananyan.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ananyan.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-03-02T17:58:59+00:00</updated><id>https://ananyan.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">3D point cloud classification of engineering designs using graph neural networks</title><link href="https://ananyan.github.io/blog/2022/findersspart2/" rel="alternate" type="text/html" title="3D point cloud classification of engineering designs using graph neural networks" /><published>2022-08-01T00:00:00+00:00</published><updated>2022-08-01T00:00:00+00:00</updated><id>https://ananyan.github.io/blog/2022/findersspart2</id><content type="html" xml:base="https://ananyan.github.io/blog/2022/findersspart2/"><![CDATA[<p>The second part of my summer school experiments were done on my own just for fun and to see what worked/did not work outside of the given challenge problems.
<!--more--></p>

<hr />

<h2 id="motivation">Motivation</h2>
<p>While I wanted to explore 3D data, I was also curious about how graph neural networks work and can apply in a design context, leading me to the <a href="https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing">Point Cloud Classification tutorial</a> in PyTorch Geometric. I decided to use the <a href="https://simjeb.github.io/">SimJEB dataset</a>, which consists of 382 jet engine bracket 3D models to test this out, though I was unsure if this would be enough data for a graph neural network.</p>

<h2 id="data">Data</h2>
<p>Each bracket, excluding those categories labeled as “other,” was converted to a point cloud using the open3d library and 2000 points sampled from the .obj file. The dataset was formatted to PyTorch Geometric data by recording the point positions and the associated bracket category to be used later for testing.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b1-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/knngraph_b1.jpg" alt="visual of block like bracket point cloud connected by edges" />

  </picture>

</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/knngraph_b2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/knngraph_b2.jpg" alt="visual of beam like bracket point cloud connected by edges" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>K-nearest neighbors clustering with k = 20 to connect nearest points with edges into a graph.</i>
</div>

<h2 id="graph-neural-network">Graph Neural Network</h2>
<p>The GNN was taken with almost no modification from the Point Cloud Classification tutorial from PyTorch Geometric. PointNet++ implementation summarized from the tutorial:</p>
<ol>
  <li>Group nodes using k-nearest neighbors as shown in the image above (or ball queries). In my case, I used k=20, somewhat arbitrarily.</li>
  <li>PointNet layers which use message passing based on the hidden features of the point at each layer and the point positions</li>
  <li>Downsampling to extract global features</li>
</ol>

<p>The downsampling is used in a normal implementation but I didn’t include it in my initial exploration and instead used the basic version that had already been implemented.</p>

<p>The neural network’s architecture is as follows: 2 PointNet layers, which take the 3 input features and map to 32 features, followed by a linear classifier that classifies the output into 5 classes. More details are available in the original PointNet++ paper as well as the tutorial I followed.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/pointnetarch-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/pointnetarch.jpg" alt="layers of neural net: sampling &amp; grouping, pointnet, sampling &amp; grouping, pointnet, output" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>PointNet++ architecture. Not my image: from the PyTorch Geometric PointNet++ tutorial.</i>  
</div>

<h2 id="training">Training</h2>
<p>I shuffled the data and split it into approximately a 90/10 ratio for training and testing. I left the default learning rate of 0.01 for the Adam optimizer, the default loss as the Cross Entropy loss, as well as the default batch size of 10. I let the model train for 50 epochs initially, but after observing that the loss was still decreasing, I decided to increase the training to 200 epochs. The initial loss was 1.5779 and by epoch 200 it was 0.4869. However, at around epoch 193, the loss was at its lowest at 0.1033 before it started to increase again. I would have to look more closely at this behavior to understand the exact problem, but it seems there was overfitting.</p>

<h2 id="outcome">Outcome</h2>
<p>The final accuracy on the test set was 0.5263 and is visualized in the confusion matrix below. Obviously, this accuracy is not very good, though with 5 classes, it is better than random guessing. Additionally, the test set classes are a bit unbalanced, with only one test set example being the arch or butterfly bracket. This is in line with the dataset, which had fewer submitted designs of this type (as they tended to have lower mass but also lower strength). I would need to use the AUC metric instead of accuracy to address this point. However, the results were somewhat expected for several reasons:</p>
<ol>
  <li>The PointNet++ implementation is not complete or tuned, having been taken directly from the tutorial.</li>
  <li>There are only hundreds of training examples instead of thousands or more.</li>
  <li>The labels in the dataset were determined qualitatively by the researchers who created the dataset. Unlikely common objects like chairs or tables in ShapeNet, there is actually no ground truth for the “types” of brackets. Therefore, it is possible that a good PointNet++ embedding could result in classification that actually makes more sense based on visual similarity than the human categorization. In this case comparing to the human labels is not a good accuracy measure.</li>
</ol>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/confmat-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/confmat-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/confmat-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/confmat.jpg" alt="full confusion matrix showing true and predicted bracket category labels. important results are summarized in the text" />

  </picture>

</figure>

    </div>
</div>

<p>I looked at the confusion matrix more closely to try to understand what the issues might be before thinking about making any changes to the network’s architecture or trying a different approach altogether. There were a couple of things to note just based on the matrix:</p>
<ol>
  <li>The block designs were categorized well, as was the single butterfly design.</li>
  <li>The flat designs were not terribly categorized, though they were often mistaken for a block design and in rarer cases, an arch or beam design.<br />
Visual inspection showed why some brackets may not have been categorized correctly, pointing towards problems when the ground truth is unknown. Below is the example of the bracket in the test set that had a true label as an arch, but was categorized as a block.</li>
</ol>

<div class="row justify-content-sm-center">
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b415-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b415-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b415-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b415.jpg" alt="Bracket 415 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b417-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b417-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b417-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b417.jpg" alt="Bracket 417 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b316-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b316-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b316-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b316.jpg" alt="Bracket 316 3D model" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>On the left is a 3D model of bracket 415. The true category was arch, but this was categorized as a block. In the middle is bracket 417, an example of an arch category bracket from the training set. On the right is bracket 316, a random example of a block category from the training set. Visually, its categorization of 415 as block does not seem to be completely off base!</i>
</div>

<p>Another example is several brackets that had true labels as flat, but were categorized as something else.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b39-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b39-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b39-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b39.jpg" alt="Bracket 39 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b28-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b28-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b28-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b28.jpg" alt="Bracket 28 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b198-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b198-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b198-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b198.jpg" alt="Bracket 198 3D model" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Left: bracket 39, classified as block; middle: bracket 28, classified as beam; right: bracket 198, classified as arch. Below are all brackets that were correctly identified as flat.</i>
</div>

<div class="row justify-content-sm-center">
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b564-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b564-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b564-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b564.jpg" alt="Bracket 564 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b205-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b205-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b205-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b205.jpg" alt="Bracket 205 3D model" />

  </picture>

</figure>

    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart2/b548-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart2/b548-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart2/b548-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart2/b548.jpg" alt="Bracket 548 3D model" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>On the left is bracket 564, in the middle is bracket 205, and on the right is bracket 548.</i>
</div>

<p>Clearly, it is difficult to distinguish the features that put a bracket in one category over another. If an unsupervised approach was taken and the brackets were clustered instead, maybe different categories would have resulted. Furthermore, it is possible that a human might agree with the results of the classifier if there are enough common features with the rest of the category that can be found by eye. Another future approach to explore for this type of data is semi-supervised learning.</p>

<h2 id="lessons-learned">Lessons learned</h2>
<p>This example from engineering design was particularly interesting because I would say the classifier appeared to work relatively fine given that the “true” labels were actually quite uncertain. In addition, I did not change the neural net’s architecture or hyperparameters so it is possible that things could be improved in that way. Of course, there is a lot left to be learned about different types of neural networks or other methods of learning a compact representation of 3D models (maybe, PointNet++ is not the best for the amount of data and the diversity in the global shape of the meshes). However, finding something that better represents this type of design-specific dataset could be useful, for example, to understand visual similarity of forms.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part II of technical concepts learned for data-driven design]]></summary></entry><entry><title type="html">3D point cloud alignment with optimal transport</title><link href="https://ananyan.github.io/blog/2022/findersspart1/" rel="alternate" type="text/html" title="3D point cloud alignment with optimal transport" /><published>2022-07-30T00:00:00+00:00</published><updated>2022-07-30T00:00:00+00:00</updated><id>https://ananyan.github.io/blog/2022/findersspart1</id><content type="html" xml:base="https://ananyan.github.io/blog/2022/findersspart1/"><![CDATA[<p><strong>What was the Frontiers in Design Representation Summer School (FinDeR)?</strong> Hosted by Prof. Mark Fuge at UMD College Park, it was a week long “summer school” where graduate students (including me) came together to learn and discuss how we represent design data for computation. The topics specific to this year’s summer school were optimal transport, information geometry, and generative models (e.g., VAEs, GANs). These topics were introduced in the context of how they may apply to data-driven design. 
<!--more--></p>

<hr />

<h2 id="motivation">Motivation</h2>
<p>I wanted to try techniques for working with 3D data since I know it is difficult to work with, yet much of design data is encoded into 3D objects such as CAD models or physical artifacts. One of the challenge problems was to try to match aorta scans between patients at different time points (younger and older) which seemed like a challenging but interesting task. Furthermore, this challenge did not have much data (only 8 .stl files) associated with it, allowing me to see how techniques might work in the face of limited data. I knew that it would not be possible to use what I had learned about generative models such as GANs with this amount of data. Furthermore, the techniques I learned from information geometry would be difficult to apply since they require knowing and being able to specify the manifold the data lies on. Thus, what I learned about optimal transport was a good option to try on this data.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/aortapointcloudgraphs-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/aortapointcloudgraphs-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/aortapointcloudgraphs-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/aortapointcloudgraphs.jpg" alt="Visual of the eight point clouds of patient aortas as described in the motivation and data sections" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Aorta point clouds in no particular order.</i>
</div>

<h2 id="data">Data</h2>
<p>Each aorta was converted to a point cloud using the open3d library and 1000 points sampled from the .stl file. Initially, I did not apply any preprocessing to the point clouds. However, as I worked on the problem, I determined that certain preprocessing methods such as global registration could probably improve the results. My teammates and I realized (at the very end) that we likely needed to apply scaling as well.</p>

<h2 id="optimal-transport">Optimal Transport</h2>
<p>This concept was introduced to us by Dr. Jean Feydy in the context of ways to transform points from source to target. More broadly, optimal transport is essentially a generalization of sorting to dimensions that are greater than one. It can be used, for example, to find the distance between unordered point clouds where you do not know the mapping between the points. To implement this idea in the context of the aorta 3D scans, I used the tutorial notebook provided by Dr. Feydy (available at <a href="https://www.jeanfeydy.com/research.html">here</a> under Talks: Frontiers in Design Representation, University of Maryland) that demonstrated optimal transport in 2D, and extended it to 3D. When applying the concept of optimal transport, the loss was defined as follows:</p>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otlossequation-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otlossequation-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otlossequation-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otlossequation.jpg" alt="Loss(x_1 to x_n; y_1 to y_n) = 1 divided by 2 times N multiplied by the minimum over permutations sigma of the sum, from i = 1 to N, of the squared distances between x_i and y_sigma(i) " />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>OT loss as defined in Dr. Feydy's tutorial. Not my image.</i>
</div>

<p>Notably, the Wasserstein distance is the square root of OT(A,B). Then, optimization needs to be performed to minimize this loss. This optimization can be conducted using gradient descent on the loss with respect to the features on the source object. In the tutorial, Dr. Feydy introduces the Sinkhorn algorithm, which I do not understand that well yet, within the optimization loop. Initially, I used the xyz-coordinates of the point cloud as the features (as an extension of the tutorial), despite knowing that it may not be the best representation for the structure of the aorta. The cost function in this case is the squared distance between the points of the two point clouds. Notably, optimal transport currently only works well for these types of simple cost functions since the pairwise costs between points have to be calculated.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otoptimization-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otoptimization-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otoptimization-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otoptimization.jpg" alt="visual of the movement of source points to target points between two aortas over timesteps 0, 0.25, 0.5, 1, 2, and 5" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Visualization of the optimization over several timesteps to find the best mapping between points of two different point clouds.</i>
</div>

<h2 id="outcome">Outcome</h2>
<p>Since the 3D .stl files were available, a visual inspection could be performed to qualitatively assign each aorta to its respective pair, though the ground truth was initially unknown.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/pairedaortas-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/pairedaortas-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/pairedaortas-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/pairedaortas.jpg" alt="3d models of aortas sorted by visually assessed pairs: 2 &amp; 5, 4 &amp; 1, 8 &amp; 7, 6 &amp; 3, where the second aorta is the smaller one" />

  </picture>

</figure>

    </div>
    <div class="col-sm-2 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/aortaexample-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/aortaexample-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/aortaexample-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/aortaexample.jpg" alt="higher resolution aorta point cloud in rainbow colors" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Left: The aorta .stl files paired according to patient based on our qualitative assessment. Right: an example of an aorta as a point cloud visualization from open3d. Not my images: both images were generated by fellow summer school attendees Leah (Michigan State) and Connor (UMD College Park), who I had several discussions with to tackle this challenge problem.</i>
</div>

<p>Then, the optimal transport loss was minimized over 100 time steps for each pair of aortas as the 1000-point point cloud as mentioned above. Below is a visualization of the optimal transport loss after 20 iterations. When the source and target were switched, the loss values were actually different (the optimization ended with a different permutation of points being mapped to each other). This is something I would have to investigate further to understand its impact on results.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otloss_noregistration.jpg" alt="full confusion matrix" />

  </picture>

</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration_tri-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration_tri-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otloss_noregistration_tri-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otloss_noregistration_tri.jpg" alt="lower triangle confusion matrix" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Left: Optimal transport loss values for initial point clouds with xyz-coordinates as features. The only pair that minimizes loss in a way that appears to match our visual assessment is 7 and 8 (unfortunately, labeled 6 and 7 here due to 0-indexing). Right: Just the lower triangle of the left image for easier visualization.</i>
</div>

<p>I then realized that some of the aortas were not aligned in 3D space, which is a problem for calculating a distance with xyz-coordinates as features. Dr. Feydy also noted that optimal transport does not generally work well if the distances are very large. To fix this issue, I decided to use open3d to perform global registration on the point clouds. This global registration is determined using fast point feature histograms, which are essentially features of each point based on its geometric neighbors. These features are matched to determine a transformation from source to target that aligns the point clouds.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/pcd_noreg-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/pcd_noreg-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/pcd_noreg-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/pcd_noreg.jpg" alt="two unaligned aorta point clouds" />

  </picture>

</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/pcd_reg-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/pcd_reg-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/pcd_reg-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/pcd_reg.jpg" alt="the same aorta point clouds aligned" />

  </picture>

</figure>

    </div>
</div>

<p>Again, the optimal transport loss was minimized and visualized, this time with the xyz-coordinates of the globally registered point clouds as features.</p>

<div class="row justify-content-sm-center">
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otloss_reg.jpg" alt="full confusion matrix" />

  </picture>

</figure>

    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg_tri-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg_tri-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/Blog/FinderPart1/otloss_reg_tri-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/Blog/FinderPart1/otloss_reg_tri.jpg" alt="lower triangle confusion matrix" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    <i>Left: Optimal transport loss values for globally registered point clouds with xyz-coordinates as features. The only pair that minimizes loss in a way that appears to match our visual assessment is 3 and 6 (unfortunately, labeled 2 and 5 here due to 0-indexing). Right: Just the lower triangle of the left image for easier visualization.</i>
</div>

<p>Finally, I attempted to address the issue of scaling by applying the iterative closest point (ICP) registration in open3d to get another transformation that includes scaling. The ICP algorithm requires an initial registration, which is satisfied by the transformation matrix obtained from global registration. However, this did not appear to make a difference in the optimal transport results.</p>

<p>Compared to our group’s qualitative assessment of which aortas looked like they would be paired, the results only corroborated one pair (7 &amp; 8) without any registration and a different pair (3 &amp; 6) after registration. However, the ground truth revealed at the end of the summer school was that the actual pairs were: 2 &amp; 8, 1 &amp; 4 (we assessed correctly), 3 &amp; 6 (we assessed correctly), and 5 &amp; 7. Looking back at the results, without global registration, only 5 &amp; 7 could have been correctly paired using optimal transport. With global registration, 2 &amp; 8, 3 &amp; 6, and 5 &amp; 7 could have been correctly paired. It was interesting how we were able to pair 1 &amp; 4 visually while the optimal transport algorithm could not, yet it was able to pair 5 &amp; 7 and 2 &amp; 8 while we could not.</p>

<h2 id="lessons-learned">Lessons learned</h2>
<p>Since the sizes of the aortas changed over the different scan times, the use of just xyz-coordinates is difficult: scaling matters. In addition, the xyz-coordinates do not capture geometric areas of the aorta that may vary across patients, but not over time. Using features that capture the general shape of the point cloud or domain-specific features (for example, the radius of specific arteries attached to the aorta) may have led to better outcomes. If there was more data, specifying the 3D features might become less important, as neural networks can be used to learn a latent representation of the object. Finally, although I learned about the concept of optimal transport, which is an interesting approach to compare two distributions, I would have to dig deeper into it to better understand its limitations compared to other approaches. Of course, both understanding and implementation is a huge barrier when trying to apply these types of advanced methods to complex problems, which is why I was limited to trying things that could be extended from existing implementations.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part I of technical concepts learned for data-driven design]]></summary></entry></feed>